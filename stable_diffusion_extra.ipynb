{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d3862-139a-4997-9b9e-cc64e9437205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c311e-326f-4a81-a14c-b7c371bfeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/devforfu/diffusion-nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e13479-1570-45b5-945f-7423763d648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -Uq diffusers transformers fastcore fastdownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7ecfc-f46b-424b-a55d-7670248018b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():\n",
    "    from huggingface_hub import notebook_login\n",
    "    if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dde48-5ab5-49dc-b43c-112e169a8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f243b0-6cb5-4bd3-8e78-95c839f928b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb67ecb-6cb2-4e81-89ff-38362db9a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DiffusionConfig:\n",
    "    vae: str = \"stabilityai/sd-vae-ft-ema\"\n",
    "    unet: str = \"CompVis/stable-diffusion-v1-4\"\n",
    "    clip_tok: str = \"openai/clip-vit-large-patch14\"\n",
    "    clip_enc: str = \"openai/clip-vit-large-patch14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413f33c-10bb-4b9c-8316-fdbfd8fdda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to(models, where): \n",
    "    return [m.to(where) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadf2ad-f281-4718-8281-173ebf59fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(cfg: DiffusionConfig, device, half=True):\n",
    "    dtype = torch.float16 if half else torch.float32\n",
    "    vae, unet, tok, enc = [\n",
    "        AutoencoderKL.from_pretrained(cfg.vae, torch_dtype=dtype),\n",
    "        UNet2DConditionModel.from_pretrained(cfg.unet, subfolder=\"unet\", torch_dtype=dtype),\n",
    "        CLIPTokenizer.from_pretrained(cfg.clip_tok, torch_dtype=dtype),\n",
    "        CLIPTextModel.from_pretrained(cfg.clip_enc, torch_dtype=dtype),\n",
    "    ]\n",
    "    vae, unet, enc = to([vae, unet, enc], device)\n",
    "    return (vae, unet, tok, enc), device, half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da0f38-0abf-4bcd-b806-eb4be3acf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Prompt:\n",
    "    positive: str\n",
    "    negative: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6fc6f-e6d2-49b9-b6ba-cbe68805060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    \n",
    "    def __init__(self, parts, device, half=True):\n",
    "        self.vae, self.unet, self.tok, self.enc = parts \n",
    "        self.device = device\n",
    "        self.half = half\n",
    "        \n",
    "    @property\n",
    "    def dtype(self): return torch.float16 if self.half else torch.float32\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_cfg(cfg, device, half=True):\n",
    "        return Diffusion(*build(cfg, device, half))\n",
    "    \n",
    "    def embed(self, prompts):\n",
    "        txt_inp = self.tok(prompts, padding=\"max_length\", max_length=self.tok.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        txt_emb = self.enc(txt_inp.input_ids.to(self.device))[0].to(self.dtype)\n",
    "        max_len = txt_inp.input_ids.shape[-1]\n",
    "        unc_inp = self.tok([\"\"] * len(prompts), padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n",
    "        unc_emb = self.enc(unc_inp.input_ids.to(self.device))[0].to(self.dtype)\n",
    "        return torch.cat([unc_emb, txt_emb])\n",
    "    \n",
    "    def latents(self, prompts, h, w):\n",
    "        latents = torch.randn((len(prompts), self.unet.in_channels, h//8, w//8))\n",
    "        latents = latents.to(self.device).to(self.dtype)\n",
    "        return latents\n",
    "    \n",
    "    def denoise(self, latents, embedded, scheduler, n_steps, g=1.0):\n",
    "        scheduler.set_timesteps(n_steps)\n",
    "        latents *= scheduler.init_noise_sigma\n",
    "        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "            inp = scheduler.scale_model_input(torch.cat([latents]*2), ts)\n",
    "            with torch.no_grad():\n",
    "                u, t = self.unet(inp, ts, encoder_hidden_states=embedded).sample.chunk(2)\n",
    "            pred = u + g*(t - u)\n",
    "            latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "        with torch.no_grad():\n",
    "            decoded = self.vae.decode(1/0.18215 * latents).sample\n",
    "        return (decoded/2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "    def generate_images(self, prompts: list[Prompt], w, h, scheduler=None, n=70, gs=1.0, seeds=1):\n",
    "        scheduler = scheduler or default_scheduler()\n",
    "        \n",
    "        if not isinstance(gs, list):\n",
    "            gs = [gs]\n",
    "            \n",
    "        if not isinstance(seeds, list):\n",
    "            seeds = [seeds]\n",
    "            \n",
    "        pil_images = []\n",
    "        print(\"processing prompts:\")\n",
    "        print(prompts)\n",
    "        \n",
    "        for seed in seeds:\n",
    "            torch.manual_seed(seed)\n",
    "            print(f\"manual seed: {seed} | g=\", end=\"\")\n",
    "            \n",
    "            for guidance in gs:\n",
    "                print(f\"{guidance}..\", end=\"\")\n",
    "                  \n",
    "                latents = self.latents(prompts, w, h)    \n",
    "                embedded = self.embed(prompts)\n",
    "                denoised = diff.denoise(latents, embedded, scheduler, n_steps=n, g=guidance)\n",
    "                arrays = torch.einsum(\"nchw->nhwc\", denoised).detach().cpu().numpy()\n",
    "                images = (arrays * 255).round().astype(np.uint8)\n",
    "                pil_images += [PIL.Image.fromarray(img) for img in images]\n",
    "                  \n",
    "            print(\"done!\")\n",
    "            \n",
    "        return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffb7e9-9570-45fa-82d1-85d9264b166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_scheduler():\n",
    "    beta_start,beta_end = 0.00085,0.012\n",
    "    num_inference_steps = 70\n",
    "    num_train_timesteps = 1000\n",
    "    return LMSDiscreteScheduler(\n",
    "        beta_start=beta_start, beta_end=beta_end, \n",
    "        beta_schedule=\"scaled_linear\", \n",
    "        num_train_timesteps=num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f9a50-0856-43d3-8f92-a12a58737121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    w,h = imgs[0].size\n",
    "    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, img in enumerate(imgs): \n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff5be5-9aba-4962-830b-34426a6562f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts, device, half = build(DiffusionConfig(), torch.device(\"cuda\"), half=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49995974-4617-4606-a37d-e386ce6d1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = Diffusion(parts, device, half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bd4b0-79b2-4408-8c26-a8a31777af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Labrador in the style of Vermeer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d61d0b-e975-45e5-aa2b-cafbf05c9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = diff.generate_images(prompts, 512, 512, seeds=[1,2,3,4], gs=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce2f12-8b46-4fbf-b258-b48c87e8aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5c658-ea40-425c-b7b8-df8667bf1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(images, rows=1, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0847a70-a98c-495b-bdda-0efae876d9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
