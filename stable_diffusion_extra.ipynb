{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d3862-139a-4997-9b9e-cc64e9437205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c311e-326f-4a81-a14c-b7c371bfeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/devforfu/diffusion-nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e13479-1570-45b5-945f-7423763d648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -Uq diffusers transformers fastcore fastdownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7ecfc-f46b-424b-a55d-7670248018b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():\n",
    "    from huggingface_hub import notebook_login\n",
    "    if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dde48-5ab5-49dc-b43c-112e169a8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f243b0-6cb5-4bd3-8e78-95c839f928b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55c3eb-1219-4790-bfd4-d1f08dec6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DiffusionConfig:\n",
    "    vae: str = \"stabilityai/sd-vae-ft-ema\"\n",
    "    unet: str = \"CompVis/stable-diffusion-v1-4\"\n",
    "    clip_tok: str = \"openai/clip-vit-large-patch14\"\n",
    "    clip_enc: str = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "def to(models, where): \n",
    "    return [m.to(where) for m in models]\n",
    "\n",
    "def build(cfg: DiffusionConfig, device, half=True):\n",
    "    dtype = torch.float16 if half else torch.float32\n",
    "    vae, unet, tok, enc = [\n",
    "        AutoencoderKL.from_pretrained(cfg.vae, torch_dtype=dtype),\n",
    "        UNet2DConditionModel.from_pretrained(cfg.unet, subfolder=\"unet\", torch_dtype=dtype),\n",
    "        CLIPTokenizer.from_pretrained(cfg.clip_tok, torch_dtype=dtype),\n",
    "        CLIPTextModel.from_pretrained(cfg.clip_enc, torch_dtype=dtype),\n",
    "    ]\n",
    "    vae, unet, enc = to([vae, unet, enc], device)\n",
    "    return (vae, unet, tok, enc), device, half\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, parts, device, half=True):\n",
    "        self.vae, self.unet, self.tok, self.enc = parts \n",
    "        self.device = device\n",
    "        self.half = half\n",
    "        \n",
    "    @property\n",
    "    def dtype(self): return torch.float16 if self.half else torch.float32\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_cfg(cfg, device, half=True):\n",
    "        return Diffusion(*build(cfg, device, half))\n",
    "    \n",
    "    def embed(self, prompts):\n",
    "        txt_inp = self.tok(prompts, padding=\"max_length\", max_length=self.tok.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        txt_emb = self.enc(txt_inp.input_ids.to(self.device))[0].to(self.dtype)\n",
    "        max_len = txt_inp.input_ids.shape[-1]\n",
    "        unc_inp = self.tok([\"\"] * len(prompts), padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n",
    "        unc_emb = self.enc(unc_inp.input_ids.to(self.device))[0].to(self.dtype)\n",
    "        return torch.cat([unc_emb, txt_emb])\n",
    "    \n",
    "    def latents(self, prompts, h, w):\n",
    "        latents = torch.randn((len(prompts), self.unet.in_channels, h//8, w//8))\n",
    "        latents = latents.to(self.device).to(self.dtype)\n",
    "        return latents\n",
    "    \n",
    "    def denoise(self, latents, embedded, scheduler, n_steps, g=1.0):\n",
    "        scheduler.set_timesteps(n_steps)\n",
    "        latents *= scheduler.init_noise_sigma\n",
    "        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "            inp = scheduler.scale_model_input(torch.cat([latents]*2), ts)\n",
    "            with torch.no_grad():\n",
    "                u, t = self.unet(inp, ts, encoder_hidden_states=embedded).sample.chunk(2)\n",
    "            pred = u + g*(t - u)\n",
    "            latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "        with torch.no_grad():\n",
    "            decoded = self.vae.decode(1/0.18215 * latents).sample\n",
    "        return (decoded/2 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    def generate_images(self, prompts, w, h, scheduler=None, n=70, g=1.0):\n",
    "        scheduler = scheduler or default_scheduler()\n",
    "        latents = self.latents(prompts, w, h)\n",
    "        embedded = self.embed(prompts)\n",
    "        denoised = diff.denoise(latents, embedded, scheduler, n_steps=n, g=g)\n",
    "        arrays = torch.einsum(\"nchw->nhwc\", denoised).detach().cpu().numpy()\n",
    "        images = (arrays * 255).round().astype(np.uint8)\n",
    "        return [PIL.Image.fromarray(img) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffb7e9-9570-45fa-82d1-85d9264b166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_scheduler():\n",
    "    beta_start,beta_end = 0.00085,0.012\n",
    "    num_inference_steps = 70\n",
    "    num_train_timesteps = 1000\n",
    "    return LMSDiscreteScheduler(\n",
    "        beta_start=beta_start, beta_end=beta_end, \n",
    "        beta_schedule=\"scaled_linear\", \n",
    "        num_train_timesteps=num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff5be5-9aba-4962-830b-34426a6562f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts, device, half = build(DiffusionConfig(), torch.device(\"cuda\"), half=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49995974-4617-4606-a37d-e386ce6d1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = Diffusion(parts, device, half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bd4b0-79b2-4408-8c26-a8a31777af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['a photograph of an astronaut riding a horse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d61d0b-e975-45e5-aa2b-cafbf05c9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = diff.generate_images(prompts, 512, 512, g=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee670c-0a58-49d3-880c-e5f845a553b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545084f-810e-4a89-bab7-e1accd81b23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
